{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/paramiko/transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' LINK ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_data = pd.read_csv('reddit.csv')\n",
    "\n",
    "selected_columns = ['id', 'title', 'selftext', 'subreddit', 'permalink']\n",
    "selected_data = reddit_data[selected_columns].copy()\n",
    "selected_data['title'] = selected_data['title'].apply(clean_text)\n",
    "selected_data['selftext'] = selected_data['selftext'].apply(clean_text)\n",
    "selected_data['permalink'] = 'https://www.reddit.com' + selected_data['permalink']\n",
    "\n",
    "selected_data['label'] = 0\n",
    "\n",
    "sampled_data = selected_data.sample(n=50)\n",
    "\n",
    "selected_data.to_csv('labeled.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160hprs</td>\n",
       "      <td>thought was reading satire</td>\n",
       "      <td>literally thought was on this sub and was wait...</td>\n",
       "      <td>CoronavirusCirclejerk</td>\n",
       "      <td>https://www.reddit.com/r/CoronavirusCirclejerk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160hnsx</td>\n",
       "      <td>exploring the intricacies of toronto summer we...</td>\n",
       "      <td>has there been noticeable impact from the wild...</td>\n",
       "      <td>askTO</td>\n",
       "      <td>https://www.reddit.com/r/askTO/comments/160hns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>160hmzi</td>\n",
       "      <td>what wildfire evacuation notices mean steps to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PortlandOregon</td>\n",
       "      <td>https://www.reddit.com/r/PortlandOregon/commen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>160hh5j</td>\n",
       "      <td>it was violent event fire chief says of wildfi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ilovebc</td>\n",
       "      <td>https://www.reddit.com/r/ilovebc/comments/160h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>160hh4y</td>\n",
       "      <td>really exoticca too soon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>https://www.reddit.com/r/facepalm/comments/160...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  160hprs                         thought was reading satire   \n",
       "1  160hnsx  exploring the intricacies of toronto summer we...   \n",
       "2  160hmzi  what wildfire evacuation notices mean steps to...   \n",
       "3  160hh5j  it was violent event fire chief says of wildfi...   \n",
       "4  160hh4y                           really exoticca too soon   \n",
       "\n",
       "                                            selftext              subreddit  \\\n",
       "0  literally thought was on this sub and was wait...  CoronavirusCirclejerk   \n",
       "1  has there been noticeable impact from the wild...                  askTO   \n",
       "2                                                NaN         PortlandOregon   \n",
       "3                                                NaN                ilovebc   \n",
       "4                                                NaN               facepalm   \n",
       "\n",
       "                                           permalink  label  \n",
       "0  https://www.reddit.com/r/CoronavirusCirclejerk...      0  \n",
       "1  https://www.reddit.com/r/askTO/comments/160hns...      0  \n",
       "2  https://www.reddit.com/r/PortlandOregon/commen...      0  \n",
       "3  https://www.reddit.com/r/ilovebc/comments/160h...      1  \n",
       "4  https://www.reddit.com/r/facepalm/comments/160...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = pd.read_csv('labeled.csv')\n",
    "labeled_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([223,  38,  76,  77,  84,  65, 229, 155,  72,  71,  37,  69,  18, 221,\n",
      "        165, 168,  22,  66, 203,  58, 192,  20,  24,  47,  21,  53,  30,  42,\n",
      "         44,  45, 246, 189, 132, 131,  99, 234,  68, 133,  95, 172,  27,  48,\n",
      "        195, 217, 158, 144,  29, 209, 235, 194]) 248 tensor(246)\n",
      "                 author  author_flair_text  clicked  comments  created_utc  \\\n",
      "0            locodallas                NaN      NaN       NaN   1692918997   \n",
      "1  JeanChretieninSpirit                NaN      NaN       NaN   1692918877   \n",
      "2      RobotTomPeterson                NaN      NaN       NaN   1692918825   \n",
      "3       PacificProvince                NaN      NaN       NaN   1692918453   \n",
      "4          floppydisk69                NaN      NaN       NaN   1692918452   \n",
      "\n",
      "   distinguished  edited       id  is_original_content  is_self  ...  saved  \\\n",
      "0            NaN     NaN  160hprs                  NaN      NaN  ...    NaN   \n",
      "1            NaN     NaN  160hnsx                  NaN      NaN  ...    NaN   \n",
      "2            NaN     NaN  160hmzi                  NaN      NaN  ...    NaN   \n",
      "3            NaN     NaN  160hh5j                  NaN      NaN  ...    NaN   \n",
      "4            NaN     NaN  160hh4y                  NaN      NaN  ...    NaN   \n",
      "\n",
      "   score                                           selftext spoiler  stickied  \\\n",
      "0    NaN  Literally thought I was on this sub and was wa...     NaN       NaN   \n",
      "1    NaN  Has there been a noticeable impact from the wi...     NaN       NaN   \n",
      "2    NaN                                                NaN     NaN       NaN   \n",
      "3    NaN                                                NaN     NaN       NaN   \n",
      "4    NaN                                                NaN     NaN       NaN   \n",
      "\n",
      "               subreddit                                              title  \\\n",
      "0  CoronavirusCirclejerk                       Thought I was reading satire   \n",
      "1                  askTO  Exploring the Intricacies of Toronto's Summer ...   \n",
      "2         PortlandOregon  What wildfire evacuation notices mean, steps t...   \n",
      "3                ilovebc  'It was a violent event,' B.C. fire chief says...   \n",
      "4               facepalm                         Really Exoticca? Too soon.   \n",
      "\n",
      "   upvote_ratio                                                url  label  \n",
      "0          0.67             https://www.reddit.com/gallery/160hprs      0  \n",
      "1          1.00  https://www.reddit.com/r/askTO/comments/160hns...      0  \n",
      "2          1.00  https://www.kgw.com/article/news/local/wildfir...      0  \n",
      "3          1.00  https://bc.ctvnews.ca/it-was-a-violent-event-b...      0  \n",
      "4          0.33                https://i.redd.it/8m71vjdq35kb1.jpg      0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "                  author  author_flair_text  clicked  comments  created_utc  \\\n",
      "243  WilliamsLakeNewsBot                NaN      NaN       NaN   1692851783   \n",
      "244          Wattsferatu                NaN      NaN       NaN   1692851670   \n",
      "245   Leatherbootsonfire                NaN      NaN       NaN   1692851547   \n",
      "246    Key-Frosting-5878                NaN      NaN       NaN   1692850983   \n",
      "247      LittleDarkOne13                NaN      NaN       NaN   1692849773   \n",
      "\n",
      "     distinguished  edited       id  is_original_content  is_self  ...  saved  \\\n",
      "243            NaN     NaN  15zrjd5                  NaN      NaN  ...    NaN   \n",
      "244            NaN     NaN  15zri2x                  NaN      NaN  ...    NaN   \n",
      "245            NaN     NaN  15zrgnk                  NaN      NaN  ...    NaN   \n",
      "246            NaN     NaN  15zra5a                  NaN      NaN  ...    NaN   \n",
      "247            NaN     NaN  15zqv5h                  NaN      NaN  ...    NaN   \n",
      "\n",
      "     score                                           selftext spoiler  \\\n",
      "243    NaN                                                NaN     NaN   \n",
      "244    NaN                                                NaN     NaN   \n",
      "245    NaN   Has anyone gone from Federal to State wildlan...     NaN   \n",
      "246    NaN  Sharing this for my hometown in need. Check it...     NaN   \n",
      "247    NaN  My husband and I were looking forward to takin...     NaN   \n",
      "\n",
      "     stickied         subreddit  \\\n",
      "243       NaN  WilliamsLakeNews   \n",
      "244       NaN        TheTikiHut   \n",
      "245       NaN          Wildfire   \n",
      "246       NaN         Louisiana   \n",
      "247       NaN           camping   \n",
      "\n",
      "                                                 title  upvote_ratio  \\\n",
      "243  Cariboo crews, businesses, supporting wildfire...          1.00   \n",
      "244                          Michael Murphy - Wildfire          0.90   \n",
      "245          Federal to state wildland fire, worth it?          0.91   \n",
      "246                                LOUISIANA WILDFIRES          0.98   \n",
      "247  AITA for wanting to cancel camping (with our b...          0.82   \n",
      "\n",
      "                                                   url  label  \n",
      "243  https://news.google.com/rss/articles/CBMiZGh0d...      0  \n",
      "244   https://youtu.be/t2YWqPhiftc?si=zzIh07D8-_wr0iI9      0  \n",
      "245  https://www.reddit.com/r/Wildfire/comments/15z...      0  \n",
      "246             https://www.reddit.com/gallery/15zra5a      0  \n",
      "247  https://www.reddit.com/r/camping/comments/15zq...      0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "labeled_data = pd.read_csv('labeled.csv')\n",
    "unlabeled_data = pd.read_csv('reddit.csv')\n",
    "unlabeled_data['label'] = 0\n",
    "\n",
    "# Define BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.iloc[idx]['title']\n",
    "        selftext = self.data.iloc[idx]['selftext']\n",
    "        text = title if isinstance(title, str) else ''  + ' ' + selftext if isinstance(selftext, str) else ''\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Define an active learning loop using uncertainty sampling\n",
    "seed_dataset = labeled_data.sample(n=99)  # Start with a small seed dataset\n",
    "train_dataset = CustomDataset(seed_dataset, tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_iterations = 5  # Number of active learning iterations\n",
    "for iteration in range(num_iterations):\n",
    "    # Train the model on the current labeled dataset\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Apply uncertainty sampling to select new examples for labeling\n",
    "    unlabeled_dataset = CustomDataset(unlabeled_data, tokenizer, max_length=128)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    uncertainty_scores = []\n",
    "    for batch in unlabeled_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "        uncertainty = torch.max(probs, dim=1).values\n",
    "        uncertainty_scores.extend(uncertainty.cpu().numpy())\n",
    "    \n",
    "    uncertainty_scores = torch.tensor(uncertainty_scores)\n",
    "    selected_indices = uncertainty_scores.argsort(descending=True)[:50]\n",
    "    selected_indices = selected_indices[selected_indices < len(unlabeled_data)]\n",
    "    print(selected_indices, len(unlabeled_data), max(selected_indices))\n",
    "    print(unlabeled_data.head())\n",
    "    print(unlabeled_data.tail())\n",
    "    selected_examples = unlabeled_data.iloc[selected_indices]\n",
    "    \n",
    "    # Add the newly labeled examples to the seed dataset\n",
    "    seed_dataset = pd.concat([seed_dataset, selected_examples])\n",
    "    \n",
    "    # Update the labeled and unlabeled datasets for the next iteration\n",
    "    labeled_data = seed_dataset\n",
    "    unlabeled_data = unlabeled_data.drop(index=selected_indices)\n",
    "    \n",
    "# Save the updated labeled dataset to next.csv\n",
    "seed_dataset.to_csv('next.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
