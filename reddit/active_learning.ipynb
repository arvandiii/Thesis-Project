{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (2.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.7.22)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install emoji\n",
    "!pip3 install transformers\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alirezaarvandi/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' LINK ', text)\n",
    "    text = re.sub(r'@\\S+', ' MENTION ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = 'reddit_0.csv'\n",
    "ouput_file_name = 'reddit_labeled_0.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_data = pd.read_csv(input_file_name)\n",
    "\n",
    "selected_columns = ['id', 'title', 'selftext', 'subreddit', 'permalink']\n",
    "selected_data = reddit_data[selected_columns].copy()\n",
    "selected_data['title'] = selected_data['title'].apply(clean_text)\n",
    "selected_data['selftext'] = selected_data['selftext'].apply(clean_text)\n",
    "selected_data['permalink'] = 'https://www.reddit.com' + selected_data['permalink']\n",
    "\n",
    "selected_data['label'] = 0\n",
    "\n",
    "sampled_data = selected_data.sample(n=10)\n",
    "\n",
    "selected_data.to_csv(ouput_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16f7d58</td>\n",
       "      <td>44 [m4f] uk/anywhere. i think i finally know w...</td>\n",
       "      <td>posting again while i try to avoid the heat h...</td>\n",
       "      <td>R4R40Plus</td>\n",
       "      <td>https://www.reddit.com/r/R4R40Plus/comments/16...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16f7cn0</td>\n",
       "      <td>argentine soldiers fire anti-aircraft missiles...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UFOs</td>\n",
       "      <td>https://www.reddit.com/r/UFOs/comments/16f7cn0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16f7c9u</td>\n",
       "      <td>tina fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u_Educational_Wind8921</td>\n",
       "      <td>https://www.reddit.com/r/u_Educational_Wind892...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16f7bio</td>\n",
       "      <td>stupid but real question</td>\n",
       "      <td>i've been a regular for 9 years now and have n...</td>\n",
       "      <td>USPS</td>\n",
       "      <td>https://www.reddit.com/r/USPS/comments/16f7bio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16f7ber</td>\n",
       "      <td>help optimising to fire</td>\n",
       "      <td>throwaway as my reddit account uses my name. i...</td>\n",
       "      <td>FIREUK</td>\n",
       "      <td>https://www.reddit.com/r/FIREUK/comments/16f7b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  16f7d58  44 [m4f] uk/anywhere. i think i finally know w...   \n",
       "1  16f7cn0  argentine soldiers fire anti-aircraft missiles...   \n",
       "2  16f7c9u                                          tina fire   \n",
       "3  16f7bio                           stupid but real question   \n",
       "4  16f7ber                            help optimising to fire   \n",
       "\n",
       "                                            selftext               subreddit  \\\n",
       "0   posting again while i try to avoid the heat h...               R4R40Plus   \n",
       "1                                                NaN                    UFOs   \n",
       "2                                                NaN  u_Educational_Wind8921   \n",
       "3  i've been a regular for 9 years now and have n...                    USPS   \n",
       "4  throwaway as my reddit account uses my name. i...                  FIREUK   \n",
       "\n",
       "                                           permalink  label  \n",
       "0  https://www.reddit.com/r/R4R40Plus/comments/16...      0  \n",
       "1  https://www.reddit.com/r/UFOs/comments/16f7cn0...      0  \n",
       "2  https://www.reddit.com/r/u_Educational_Wind892...      0  \n",
       "3  https://www.reddit.com/r/USPS/comments/16f7bio...      0  \n",
       "4  https://www.reddit.com/r/FIREUK/comments/16f7b...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = pd.read_csv(ouput_file_name)\n",
    "labeled_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([223,  38,  76,  77,  84,  65, 229, 155,  72,  71,  37,  69,  18, 221,\n",
      "        165, 168,  22,  66, 203,  58, 192,  20,  24,  47,  21,  53,  30,  42,\n",
      "         44,  45, 246, 189, 132, 131,  99, 234,  68, 133,  95, 172,  27,  48,\n",
      "        195, 217, 158, 144,  29, 209, 235, 194]) 248 tensor(246)\n",
      "                 author  author_flair_text  clicked  comments  created_utc  \\\n",
      "0            locodallas                NaN      NaN       NaN   1692918997   \n",
      "1  JeanChretieninSpirit                NaN      NaN       NaN   1692918877   \n",
      "2      RobotTomPeterson                NaN      NaN       NaN   1692918825   \n",
      "3       PacificProvince                NaN      NaN       NaN   1692918453   \n",
      "4          floppydisk69                NaN      NaN       NaN   1692918452   \n",
      "\n",
      "   distinguished  edited       id  is_original_content  is_self  ...  saved  \\\n",
      "0            NaN     NaN  160hprs                  NaN      NaN  ...    NaN   \n",
      "1            NaN     NaN  160hnsx                  NaN      NaN  ...    NaN   \n",
      "2            NaN     NaN  160hmzi                  NaN      NaN  ...    NaN   \n",
      "3            NaN     NaN  160hh5j                  NaN      NaN  ...    NaN   \n",
      "4            NaN     NaN  160hh4y                  NaN      NaN  ...    NaN   \n",
      "\n",
      "   score                                           selftext spoiler  stickied  \\\n",
      "0    NaN  Literally thought I was on this sub and was wa...     NaN       NaN   \n",
      "1    NaN  Has there been a noticeable impact from the wi...     NaN       NaN   \n",
      "2    NaN                                                NaN     NaN       NaN   \n",
      "3    NaN                                                NaN     NaN       NaN   \n",
      "4    NaN                                                NaN     NaN       NaN   \n",
      "\n",
      "               subreddit                                              title  \\\n",
      "0  CoronavirusCirclejerk                       Thought I was reading satire   \n",
      "1                  askTO  Exploring the Intricacies of Toronto's Summer ...   \n",
      "2         PortlandOregon  What wildfire evacuation notices mean, steps t...   \n",
      "3                ilovebc  'It was a violent event,' B.C. fire chief says...   \n",
      "4               facepalm                         Really Exoticca? Too soon.   \n",
      "\n",
      "   upvote_ratio                                                url  label  \n",
      "0          0.67             https://www.reddit.com/gallery/160hprs      0  \n",
      "1          1.00  https://www.reddit.com/r/askTO/comments/160hns...      0  \n",
      "2          1.00  https://www.kgw.com/article/news/local/wildfir...      0  \n",
      "3          1.00  https://bc.ctvnews.ca/it-was-a-violent-event-b...      0  \n",
      "4          0.33                https://i.redd.it/8m71vjdq35kb1.jpg      0  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "                  author  author_flair_text  clicked  comments  created_utc  \\\n",
      "243  WilliamsLakeNewsBot                NaN      NaN       NaN   1692851783   \n",
      "244          Wattsferatu                NaN      NaN       NaN   1692851670   \n",
      "245   Leatherbootsonfire                NaN      NaN       NaN   1692851547   \n",
      "246    Key-Frosting-5878                NaN      NaN       NaN   1692850983   \n",
      "247      LittleDarkOne13                NaN      NaN       NaN   1692849773   \n",
      "\n",
      "     distinguished  edited       id  is_original_content  is_self  ...  saved  \\\n",
      "243            NaN     NaN  15zrjd5                  NaN      NaN  ...    NaN   \n",
      "244            NaN     NaN  15zri2x                  NaN      NaN  ...    NaN   \n",
      "245            NaN     NaN  15zrgnk                  NaN      NaN  ...    NaN   \n",
      "246            NaN     NaN  15zra5a                  NaN      NaN  ...    NaN   \n",
      "247            NaN     NaN  15zqv5h                  NaN      NaN  ...    NaN   \n",
      "\n",
      "     score                                           selftext spoiler  \\\n",
      "243    NaN                                                NaN     NaN   \n",
      "244    NaN                                                NaN     NaN   \n",
      "245    NaN   Has anyone gone from Federal to State wildlan...     NaN   \n",
      "246    NaN  Sharing this for my hometown in need. Check it...     NaN   \n",
      "247    NaN  My husband and I were looking forward to takin...     NaN   \n",
      "\n",
      "     stickied         subreddit  \\\n",
      "243       NaN  WilliamsLakeNews   \n",
      "244       NaN        TheTikiHut   \n",
      "245       NaN          Wildfire   \n",
      "246       NaN         Louisiana   \n",
      "247       NaN           camping   \n",
      "\n",
      "                                                 title  upvote_ratio  \\\n",
      "243  Cariboo crews, businesses, supporting wildfire...          1.00   \n",
      "244                          Michael Murphy - Wildfire          0.90   \n",
      "245          Federal to state wildland fire, worth it?          0.91   \n",
      "246                                LOUISIANA WILDFIRES          0.98   \n",
      "247  AITA for wanting to cancel camping (with our b...          0.82   \n",
      "\n",
      "                                                   url  label  \n",
      "243  https://news.google.com/rss/articles/CBMiZGh0d...      0  \n",
      "244   https://youtu.be/t2YWqPhiftc?si=zzIh07D8-_wr0iI9      0  \n",
      "245  https://www.reddit.com/r/Wildfire/comments/15z...      0  \n",
      "246             https://www.reddit.com/gallery/15zra5a      0  \n",
      "247  https://www.reddit.com/r/camping/comments/15zq...      0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "labeled_data = pd.read_csv('labeled.csv')\n",
    "unlabeled_data = pd.read_csv('reddit.csv')\n",
    "unlabeled_data['label'] = 0\n",
    "\n",
    "# Define BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.iloc[idx]['title']\n",
    "        selftext = self.data.iloc[idx]['selftext']\n",
    "        text = title if isinstance(title, str) else ''  + ' ' + selftext if isinstance(selftext, str) else ''\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Define an active learning loop using uncertainty sampling\n",
    "seed_dataset = labeled_data.sample(n=99)  # Start with a small seed dataset\n",
    "train_dataset = CustomDataset(seed_dataset, tokenizer, max_length=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_iterations = 5  # Number of active learning iterations\n",
    "for iteration in range(num_iterations):\n",
    "    # Train the model on the current labeled dataset\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Apply uncertainty sampling to select new examples for labeling\n",
    "    unlabeled_dataset = CustomDataset(unlabeled_data, tokenizer, max_length=128)\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    uncertainty_scores = []\n",
    "    for batch in unlabeled_loader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "        probs = softmax(logits, dim=1)\n",
    "        uncertainty = torch.max(probs, dim=1).values\n",
    "        uncertainty_scores.extend(uncertainty.cpu().numpy())\n",
    "    \n",
    "    uncertainty_scores = torch.tensor(uncertainty_scores)\n",
    "    selected_indices = uncertainty_scores.argsort(descending=True)[:50]\n",
    "    selected_indices = selected_indices[selected_indices < len(unlabeled_data)]\n",
    "    print(selected_indices, len(unlabeled_data), max(selected_indices))\n",
    "    print(unlabeled_data.head())\n",
    "    print(unlabeled_data.tail())\n",
    "    selected_examples = unlabeled_data.iloc[selected_indices]\n",
    "    \n",
    "    # Add the newly labeled examples to the seed dataset\n",
    "    seed_dataset = pd.concat([seed_dataset, selected_examples])\n",
    "    \n",
    "    # Update the labeled and unlabeled datasets for the next iteration\n",
    "    labeled_data = seed_dataset\n",
    "    unlabeled_data = unlabeled_data.drop(index=selected_indices)\n",
    "    \n",
    "# Save the updated labeled dataset to next.csv\n",
    "seed_dataset.to_csv('next.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
