{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install emoji\n",
    "!pip3 install transformers\n",
    "!pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = 'reddit_filter.csv'\n",
    "ouput_file_name = 'reddit_labeled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reddit_data = pd.read_csv(input_file_name)\n",
    "\n",
    "selected_data = reddit_data.copy()\n",
    "selected_data['label'] = 0\n",
    "\n",
    "sampled_data = selected_data.sample(n=50)\n",
    "\n",
    "sampled_data.to_csv(ouput_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>permalink</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14eo63g</td>\n",
       "      <td>williams lake fire department to practice hazm...</td>\n",
       "      <td></td>\n",
       "      <td>WilliamsLakeNews</td>\n",
       "      <td>https://www.reddit.com/r/WilliamsLakeNews/comm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132ppze</td>\n",
       "      <td>update: wildfire burning in the chilcotin west...</td>\n",
       "      <td></td>\n",
       "      <td>WilliamsLakeNews</td>\n",
       "      <td>https://www.reddit.com/r/WilliamsLakeNews/comm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137u4hk</td>\n",
       "      <td>parks canada prescribed burn in banff becomes ...</td>\n",
       "      <td></td>\n",
       "      <td>canada</td>\n",
       "      <td>https://www.reddit.com/r/canada/comments/137u4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15bndew</td>\n",
       "      <td>big building fire in east van</td>\n",
       "      <td>looks like it's somewhere near guelph park. ho...</td>\n",
       "      <td>vancouver</td>\n",
       "      <td>https://www.reddit.com/r/vancouver/comments/15...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15hbe5d</td>\n",
       "      <td>vancouver police investigate stanley park fire...</td>\n",
       "      <td></td>\n",
       "      <td>vancouver</td>\n",
       "      <td>https://www.reddit.com/r/vancouver/comments/15...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              title  \\\n",
       "0  14eo63g  williams lake fire department to practice hazm...   \n",
       "1  132ppze  update: wildfire burning in the chilcotin west...   \n",
       "2  137u4hk  parks canada prescribed burn in banff becomes ...   \n",
       "3  15bndew                      big building fire in east van   \n",
       "4  15hbe5d  vancouver police investigate stanley park fire...   \n",
       "\n",
       "                                            selftext         subreddit  \\\n",
       "0                                                     WilliamsLakeNews   \n",
       "1                                                     WilliamsLakeNews   \n",
       "2                                                               canada   \n",
       "3  looks like it's somewhere near guelph park. ho...         vancouver   \n",
       "4                                                            vancouver   \n",
       "\n",
       "                                           permalink  label  \n",
       "0  https://www.reddit.com/r/WilliamsLakeNews/comm...      0  \n",
       "1  https://www.reddit.com/r/WilliamsLakeNews/comm...      0  \n",
       "2  https://www.reddit.com/r/canada/comments/137u4...      0  \n",
       "3  https://www.reddit.com/r/vancouver/comments/15...      0  \n",
       "4  https://www.reddit.com/r/vancouver/comments/15...      0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = pd.read_csv(ouput_file_name)\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alireza/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "labeled_data_0 = pd.read_csv('reddit_labeled_0.csv')\n",
    "labeled_data_1 = pd.read_csv('reddit_labeled_1.csv')\n",
    "labeled_data = pd.concat([labeled_data_0, labeled_data_1])\n",
    "unlabeled_data = pd.read_csv('reddit_clean.csv')\n",
    "unlabeled_data['label'] = 0\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = self.data.iloc[idx]['title']\n",
    "        selftext = self.data.iloc[idx]['selftext']\n",
    "        text = title if isinstance(title, str) else ''  + ' ' + selftext if isinstance(selftext, str) else ''\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "seed_dataset = labeled_data.sample(n=min(len(labeled_data), 50))\n",
    "train_dataset = CustomDataset(seed_dataset, tokenizer, max_length=512)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['label']\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "unlabeled_dataset = CustomDataset(unlabeled_data, tokenizer, max_length=512)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "uncertainty_scores = []\n",
    "for batch in unlabeled_loader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "    probs = softmax(logits, dim=1)\n",
    "    uncertainty = torch.max(probs, dim=1).values\n",
    "    uncertainty_scores.extend(uncertainty.cpu().numpy())\n",
    "\n",
    "uncertainty_scores = torch.tensor(uncertainty_scores)\n",
    "selected_indices = uncertainty_scores.argsort(descending=True)[:50]\n",
    "selected_indices = selected_indices[selected_indices < len(unlabeled_data)]\n",
    "selected_examples = unlabeled_data.iloc[selected_indices]\n",
    "\n",
    "selected_examples.to_csv('next.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
